## Toy Practices with langchain

### chatting streamlit applications
* you can chat with [Ollama](https://github.com/ollama/ollama) models locally.
* `chat_with_arxiv`
  * generates a response based on the context of the arxiv paper.
* `chat_with_repository`
  * generates a response based on the context of the Github repository.
